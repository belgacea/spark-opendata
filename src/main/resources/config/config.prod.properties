# Spark
spark.master=yarn
# Hypothesis : 3 nodes with 8 cpus & 32go each = 24 cpus & 96go
# Current allocation : 3 nodes with 7 cpus & 24go each = 21 cpus & 72go
# Source : https://www.rea-group.com/blog/how-we-optimize-apache-spark-apps/
# 3 cpu per instance
spark.executor.cores=2
# 3 nodes with 4 instances (+ spark driver) => 3 * 4 = 12
spark.executor.instances=12
# By default : spark.yarn.executor.memoryOverhead = spark.executor.memory * 0.10
# & spark.executor.memory + spark.yarn.executor.memoryOverhead = memory per node / number of executors per node
# <=> spark.executor.memory = memory per node / number of executors (instances) per node / 1.10
# spark.executor.memory = 28g / 4 / 1.10 ~= 6g
spark.executor.memory=6g
spark.executor.memoryOverhead=1g
spark.driver.memory=1g
spark.default.parallelism=48
spark.network.timeout=1m
spark.locality.wait=1s
spark.sql.session.timeZone=UTC
#spark.debug.maxToStringFields=100
#spark.serializer=org.apache.spark.serializer.KryoSerializer
##spark.rdd.compress=true
# General
spark.example.debug.execution.plan=false
spark.example.persistence.cache=true
spark.example.persistence.storage.level=MEMORY_AND_DISK_SER
spark.example.csv.delimiter=;
spark.example.csv.charset=utf-8